{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313de142-ec6d-43da-9bc5-02c7772d721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<1.25,>=1.24 in /opt/conda/lib/python3.11/site-packages (1.24.4)\n",
      "Requirement already satisfied: kagglehub in /opt/conda/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from kagglehub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from kagglehub) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from kagglehub) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<1.25,>=1.24\"\n",
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0324b8-0bfa-43ef-be27-ba856f0e798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/jovyan/.cache/kagglehub/datasets/hassan06/nslkdd/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import os\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"hassan06/nslkdd\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ceec90-1d6d-4d13-87d4-b781d5e3d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"ids\").getOrCreate()\n",
    "# Step 2: Load the data\n",
    "#data = spark.read.csv(path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3acd17da-98a2-4365-a926-b124c6f09a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      " |-- _c29: string (nullable = true)\n",
      " |-- _c30: string (nullable = true)\n",
      " |-- _c31: string (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      " |-- _c34: string (nullable = true)\n",
      " |-- _c35: string (nullable = true)\n",
      " |-- _c36: string (nullable = true)\n",
      " |-- _c37: string (nullable = true)\n",
      " |-- _c38: string (nullable = true)\n",
      " |-- _c39: string (nullable = true)\n",
      " |-- _c40: string (nullable = true)\n",
      " |-- _c41: string (nullable = true)\n",
      " |-- _c42: string (nullable = true)\n",
      "\n",
      "_c0 duration\n",
      "_c1 protocol_type\n",
      "_c2 service\n",
      "_c3 flag\n",
      "_c4 src_bytes\n",
      "_c5 dst_bytes\n",
      "_c6 land\n",
      "_c7 wrong_fragment\n",
      "_c8 urgent\n",
      "_c9 hot\n",
      "_c10 num_failed_logins\n",
      "_c11 logged_in\n",
      "_c12 num_compromised\n",
      "_c13 root_shell\n",
      "_c14 su_attempted\n",
      "_c15 num_root\n",
      "_c16 num_file_creations\n",
      "_c17 num_shells\n",
      "_c18 num_access_files\n",
      "_c19 num_outbound_cmds\n",
      "_c20 is_host_login\n",
      "_c21 is_guest_login\n",
      "_c22 count\n",
      "_c23 srv_count\n",
      "_c24 serror_rate\n",
      "_c25 srv_serror_rate\n",
      "_c26 rerror_rate\n",
      "_c27 srv_rerror_rate\n",
      "_c28 same_srv_rate\n",
      "_c29 diff_srv_rate\n",
      "_c30 srv_diff_host_rate\n",
      "_c31 dst_host_count\n",
      "_c32 dst_host_srv_count\n",
      "_c33 dst_host_same_srv_rate\n",
      "_c34 dst_host_diff_srv_rate\n",
      "_c35 dst_host_same_src_port_rate\n",
      "_c36 dst_host_srv_diff_host_rate\n",
      "_c37 dst_host_serror_rate\n",
      "_c38 dst_host_srv_serror_rate\n",
      "_c39 dst_host_rerror_rate\n",
      "_c40 dst_host_srv_rerror_rate\n",
      "_c41 outcome\n",
      "_c42 level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_file = os.path.join(path, \"KDDTrain+.txt\")\n",
    "test_file = os.path.join(path, \"KDDTest+.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3️⃣ Leggi file — ATTENZIONE: non c’è header, separatore=',' (oppure '\\t' se errore)\n",
    "cols = ['duration','protocol_type','service','flag','src_bytes','dst_bytes','land','wrong_fragment','urgent','hot'\n",
    ",'num_failed_logins','logged_in','num_compromised','root_shell','su_attempted','num_root','num_file_creations'\n",
    ",'num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login','count','srv_count','serror_rate'\n",
    ",'srv_serror_rate','rerror_rate','srv_rerror_rate','same_srv_rate','diff_srv_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count'\n",
    ",'dst_host_same_srv_rate','dst_host_diff_srv_rate','dst_host_same_src_port_rate','dst_host_srv_diff_host_rate','dst_host_serror_rate'\n",
    ",'dst_host_srv_serror_rate','dst_host_rerror_rate','dst_host_srv_rerror_rate','outcome','level']\n",
    "\n",
    "train_df = spark.read.csv(train_file, header=False, sep=',')\n",
    "train_df.printSchema()\n",
    "\n",
    "test_df  = spark.read.csv(test_file, header=False, sep=',')\n",
    "\n",
    "#print(len(cols))\n",
    "\n",
    "for old, new in zip(train_df.columns, cols):\n",
    "    print(old,new)\n",
    "    train_df=train_df.withColumnRenamed(old, new)\n",
    "\n",
    "for old, new in zip(test_df.columns, cols):\n",
    "    test_df = test_df.withColumnRenamed(old, new)\n",
    "\n",
    "\n",
    "\n",
    "#print(f\"Train: {train_df.count()}  Test: {test_df.count()}\")\n",
    "#print(train_df.columns)\n",
    "#print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23d345f-9f99-48c1-acf9-ec192382ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_numeric_columns(df, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Converte automaticamente le colonne numeriche da string a DoubleType.\n",
    "    Esclude le colonne categoriche indicate in exclude_cols.\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "\n",
    "    # Crea una copia per sicurezza\n",
    "    df_casted = df\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        if col_name not in exclude_cols:\n",
    "            df_casted = df_casted.withColumn(col_name, F.col(col_name).cast(DoubleType()))\n",
    "\n",
    "    return df_casted\n",
    "\n",
    "\n",
    "\n",
    "categorical_cols = [\"protocol_type\", \"service\", \"flag\", \"outcome\"]\n",
    "train_df=cast_numeric_columns(train_df,categorical_cols)\n",
    "test_df=cast_numeric_columns(test_df,categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367805c7-1c2e-4f74-bf19-14804210258f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('duration', 'double'),\n",
       " ('protocol_type', 'string'),\n",
       " ('service', 'string'),\n",
       " ('flag', 'string'),\n",
       " ('src_bytes', 'double'),\n",
       " ('dst_bytes', 'double'),\n",
       " ('land', 'double'),\n",
       " ('wrong_fragment', 'double'),\n",
       " ('urgent', 'double'),\n",
       " ('hot', 'double'),\n",
       " ('num_failed_logins', 'double'),\n",
       " ('logged_in', 'double'),\n",
       " ('num_compromised', 'double'),\n",
       " ('root_shell', 'double'),\n",
       " ('su_attempted', 'double'),\n",
       " ('num_root', 'double'),\n",
       " ('num_file_creations', 'double'),\n",
       " ('num_shells', 'double'),\n",
       " ('num_access_files', 'double'),\n",
       " ('num_outbound_cmds', 'double'),\n",
       " ('is_host_login', 'double'),\n",
       " ('is_guest_login', 'double'),\n",
       " ('count', 'double'),\n",
       " ('srv_count', 'double'),\n",
       " ('serror_rate', 'double'),\n",
       " ('srv_serror_rate', 'double'),\n",
       " ('rerror_rate', 'double'),\n",
       " ('srv_rerror_rate', 'double'),\n",
       " ('same_srv_rate', 'double'),\n",
       " ('diff_srv_rate', 'double'),\n",
       " ('srv_diff_host_rate', 'double'),\n",
       " ('dst_host_count', 'double'),\n",
       " ('dst_host_srv_count', 'double'),\n",
       " ('dst_host_same_srv_rate', 'double'),\n",
       " ('dst_host_diff_srv_rate', 'double'),\n",
       " ('dst_host_same_src_port_rate', 'double'),\n",
       " ('dst_host_srv_diff_host_rate', 'double'),\n",
       " ('dst_host_serror_rate', 'double'),\n",
       " ('dst_host_srv_serror_rate', 'double'),\n",
       " ('dst_host_rerror_rate', 'double'),\n",
       " ('dst_host_srv_rerror_rate', 'double'),\n",
       " ('outcome', 'string'),\n",
       " ('level', 'double')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "416d6264-5626-4e1b-ac30-c8b2567de04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = train_df.withColumn(\n",
    "    \"outcome\",\n",
    "    when(col(\"outcome\") == \"normal\", 0).otherwise(1)\n",
    ")\n",
    "\n",
    "test_df = test_df.withColumn(\n",
    "    \"outcome\",\n",
    "    when(col(\"outcome\") == \"normal\", 0).otherwise(1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09f9d1fc-19b0-42ea-ad98-a39356fbf184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|outcome|\n",
      "+-------+\n",
      "|      1|\n",
      "|      0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select(\"outcome\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2baf865-711c-4a2f-a15c-9c947b3726ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_df[\"outcome\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "996ea17e-8a91-4309-bf51-c35436a7a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "#\n",
    "test_df=test_df.drop(\"protocol_type\", \"service\", \"flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "225d5da7-c84e-40db-b7e9-ded939515007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'src_bytes',\n",
       " 'dst_bytes',\n",
       " 'land',\n",
       " 'wrong_fragment',\n",
       " 'urgent',\n",
       " 'hot',\n",
       " 'num_failed_logins',\n",
       " 'logged_in',\n",
       " 'num_compromised',\n",
       " 'root_shell',\n",
       " 'su_attempted',\n",
       " 'num_root',\n",
       " 'num_file_creations',\n",
       " 'num_shells',\n",
       " 'num_access_files',\n",
       " 'num_outbound_cmds',\n",
       " 'is_host_login',\n",
       " 'is_guest_login',\n",
       " 'count',\n",
       " 'srv_count',\n",
       " 'serror_rate',\n",
       " 'srv_serror_rate',\n",
       " 'rerror_rate',\n",
       " 'srv_rerror_rate',\n",
       " 'same_srv_rate',\n",
       " 'diff_srv_rate',\n",
       " 'srv_diff_host_rate',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_same_srv_rate',\n",
       " 'dst_host_diff_srv_rate',\n",
       " 'dst_host_same_src_port_rate',\n",
       " 'dst_host_srv_diff_host_rate',\n",
       " 'dst_host_serror_rate',\n",
       " 'dst_host_srv_serror_rate',\n",
       " 'dst_host_rerror_rate',\n",
       " 'dst_host_srv_rerror_rate',\n",
       " 'outcome',\n",
       " 'level']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a30420c3-dcaa-4c2f-9c42-ae299377e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [c for c in train_df.columns if c not in [\"protocol_type\",\"service\",\"flag\",\"outcome\"]]\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "train_vec = assembler.transform(train_df)\n",
    "test_vec  = assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b70afce0-2e0c-4b76-8480-dd6d33a8a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol  = \"features_vec\",\n",
    "    outputCol = \"features_scaled\",\n",
    "    withMean  = True,   # opzionale: sottrae la media\n",
    "    withStd   = True    # opzionale: divide per deviazione standard\n",
    ")\n",
    "scaler_model = scaler.fit(train_df_vect)\n",
    "train_df_scaled = scaler_model.transform(train_df_vect)\n",
    "\n",
    "scaler_model = scaler.fit(test_df_vect)\n",
    "test_df_scaled = scaler_model.transform(test_df_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2c179-4f28-4a21-9634-028b0e2c9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_scaled.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22c86ed2-d1b5-4fb8-b8e7-dc6242405bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"outcome\", maxIter=20)\n",
    "lr_model = lr.fit(train_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34496e69-7425-451d-a6a0-b6cf53fd65c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n",
      "|outcome|prediction|         probability|\n",
      "+-------+----------+--------------------+\n",
      "|      1|       1.0|[3.38214726403747...|\n",
      "|      1|       1.0|[4.67358635625920...|\n",
      "|      0|       0.0|[0.99999984857438...|\n",
      "|      1|       1.0|[2.46219404659849...|\n",
      "|      1|       1.0|[6.86015216342227...|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "AUC: 0.999999614832984\n"
     ]
    }
   ],
   "source": [
    "preds = lr_model.transform(test_df_scaled)\n",
    "preds.select(\"outcome\", \"prediction\", \"probability\").show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"outcome\")\n",
    "auc = evaluator.evaluate(preds)\n",
    "print(\"AUC:\", auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
