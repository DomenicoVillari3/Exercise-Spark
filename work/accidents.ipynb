{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a68f61d",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91ac5309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ce02f",
   "metadata": {},
   "source": [
    "# Imports and Sparksession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24bfc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import os\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.stat import Correlation\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5560e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spark = SparkSession.builder.appName(\"accidents\").getOrCreate()\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"accidents\") \\\n",
    "#     .config(\"spark.driver.memory\", \"8g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"8g\") \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "#     .getOrCreate()\n",
    "   \n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"accidents\")\n",
    "    .master(\"spark://spark-master:7077\")    # usa davvero il cluster\n",
    "    .config(\"spark.driver.memory\", \"6g\")     # Jupyter driver\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  \n",
    "    .config(\"spark.executor.memory\", \"6g\")   # ogni executor usa 6GB\n",
    "    .config(\"spark.executor.cores\", \"2\")     # corrisponde ai tuoi worker\n",
    "    .config(\"spark.executor.instances\", \"2\") # 1 executor per worker\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \n",
    "    .config(\"spark.default.parallelism\", \"200\")\n",
    "    .getOrCreate()\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20844af",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o39.read.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:699)\n\tat org.apache.spark.sql.SparkSession.read(SparkSession.scala:783)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accidents \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets/US_Accidents_March23.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1706\u001b[0m, in \u001b[0;36mSparkSession.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrameReader:\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;124;03m    Returns a :class:`DataFrameReader` that can be used to read data\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;124;03m    in as a :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;124;03m    +---+------------+\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:70\u001b[0m, in \u001b[0;36mDataFrameReader.__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, spark: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m spark\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o39.read.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:699)\n\tat org.apache.spark.sql.SparkSession.read(SparkSession.scala:783)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "accidents = spark.read.csv(\"datasets/US_Accidents_March23.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3290b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44ebe5",
   "metadata": {},
   "source": [
    "## show columns and initial ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "faa46673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accidents.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d40170a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(accidents.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6599a064",
   "metadata": {},
   "source": [
    "## plot labels (severity distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea3964c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1 Raggruppa severity (Spark → veloce su 7M righe)\\nseverity_counts = (\\n    accidents.groupBy(\"severity\")\\n             .count()\\n             .orderBy(\"severity\")\\n)\\n\\n# 2️ Converte in Pandas (poche righe → ok)\\npdf = severity_counts.toPandas()\\n\\n# 3️Calcolo percentuali\\npdf[\"percent\"] = pdf[\"count\"] / pdf[\"count\"].sum() * 100\\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# 1 Raggruppa severity (Spark → veloce su 7M righe)\n",
    "severity_counts = (\n",
    "    accidents.groupBy(\"severity\")\n",
    "             .count()\n",
    "             .orderBy(\"severity\")\n",
    ")\n",
    "\n",
    "# 2️ Converte in Pandas (poche righe → ok)\n",
    "pdf = severity_counts.toPandas()\n",
    "\n",
    "# 3️Calcolo percentuali\n",
    "pdf[\"percent\"] = pdf[\"count\"] / pdf[\"count\"].sum() * 100\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1aa776d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(pdf[\"percent\"])\\n# 4️⃣ Grafico a torta\\nplt.figure(figsize=(10, 8))\\nplt.pie(\\n    pdf[\"count\"],\\n    labels=pdf[\"severity\"],\\n    autopct=lambda p: f\"{p:.1f}%\",    # percentuali con 1 decimale\\n    startangle=90,\\n    pctdistance=0.8\\n)\\nplt.title(\"Distribuzione percentuale di \\'severity\\'\")\\nplt.axis(\"equal\")  # mantiene il cerchio perfettamente rotondo\\nplt.show()'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(pdf[\"percent\"])\n",
    "# 4️⃣ Grafico a torta\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(\n",
    "    pdf[\"count\"],\n",
    "    labels=pdf[\"severity\"],\n",
    "    autopct=lambda p: f\"{p:.1f}%\",    # percentuali con 1 decimale\n",
    "    startangle=90,\n",
    "    pctdistance=0.8\n",
    ")\n",
    "plt.title(\"Distribuzione percentuale di 'severity'\")\n",
    "plt.axis(\"equal\")  # mantiene il cerchio perfettamente rotondo\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd38fce",
   "metadata": {},
   "source": [
    "## Fill NaN with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d71c4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_temp = accidents.select(F.mean(\"Temperature(F)\")).first()[0]\n",
    "\n",
    "accidents = accidents.fillna({\"Temperature(F)\": mean_temp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85024464",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_numeric = ['Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)','Precipitation(in)']\n",
    "    \n",
    "for col in weather_numeric:\n",
    "    mean = accidents.select(F.mean(col)).first()[0]\n",
    "    accidents = accidents.fillna({col: mean})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d70c6",
   "metadata": {},
   "source": [
    "## Drop unuseful columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072918c",
   "metadata": {},
   "source": [
    "End_* columns are removed because aren't useful to predict the severity (can be defined only afterwards the incident)\n",
    "Weather_Timestamp is a value near the initial timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5c90d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accidents = accidents.drop('Astronomical_Twilight', 'Nautical_Twilight','Country')\n",
    "accidents = accidents.drop('ID', \"Source\",'Description', 'Distance', 'End_Time', 'Distance(mi)', 'End_Lng', 'End_Lat', 'Duration',\"Weather_Timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e62fb49",
   "metadata": {},
   "source": [
    "## define categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87780321",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = ['Street', 'City', 'County', 'State', \"Zipcode\",\n",
    "             'Timezone', 'Airport_Code', 'Wind_Direction',\n",
    "              'Weather_Condition','Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight',\n",
    "            \"Sunrise_Sunset\",\"Country\",\"Turning_Loop\",\"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\", \"Junction\", \"No_Exit\", \"Railway\", \n",
    "            \"Roundabout\", \"Station\",\"Stop\",\"Traffic_Calming\",\n",
    "             \"Traffic_Signal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f8525",
   "metadata": {},
   "source": [
    "## analyze categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f9fb330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Unique count of categorical features:\")\\nfor col in cat_names:\\n    count_unique = accidents.select(F.countDistinct(col)).first()[0]\\n    print(f\"{col}: {count_unique}\")'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"Unique count of categorical features:\")\n",
    "for col in cat_names:\n",
    "    count_unique = accidents.select(F.countDistinct(col)).first()[0]\n",
    "    print(f\"{col}: {count_unique}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd6300",
   "metadata": {},
   "source": [
    "## Drop categorical with 1 unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62f609af",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = accidents.drop(\"Country\",\"Turning_Loop\")\n",
    "cat_names.remove(\"Country\")\n",
    "cat_names.remove(\"Turning_Loop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f019ac8",
   "metadata": {},
   "source": [
    "## simplify wind direction \n",
    "from 24 values to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1e84d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\"Wind Direction: \")\\naccidents.select(\"Wind_Direction\").distinct().show()'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(\"Wind Direction: \")\n",
    "accidents.select(\"Wind_Direction\").distinct().show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "994c3157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind Direction after simplification: \n"
     ]
    }
   ],
   "source": [
    "wind_dir_mapping = {\n",
    "    \"North\": 'N',\n",
    "    'NNE': 'NE',\n",
    "    'NE': 'NE',\n",
    "    'ENE': 'E',\n",
    "    'East': 'E',\n",
    "    'ESE': 'SE',\n",
    "    'SE': 'SE',\n",
    "    'SSE': 'S',\n",
    "    'South': 'S',\n",
    "    'SSW': 'SW',\n",
    "    'SW': 'SW',\n",
    "    'WSW': 'W',\n",
    "    'West': 'W',\n",
    "    'WNW': 'NW',\n",
    "    'NW': 'NW',\n",
    "    'NNW': 'NW',\n",
    "    \"Calm\":\"CALM\",\n",
    "    \"Variable\":\"VAR\"\n",
    "}\n",
    "accidents = accidents.replace(wind_dir_mapping, subset=['Wind_Direction'])\n",
    "print(\"Wind Direction after simplification: \")\n",
    "#accidents.select(\"Wind_Direction\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba0781",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## simplify wehater_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4cd3a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accidents.select(\"Weather_Condition\").distinct().show(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d518ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    \"Clear\": \"Clear\",\n",
    "    \"Cloud\": \"Cloud|Overcast\",\n",
    "    \"Rain\": \"Rain|storm\",\n",
    "    \"Heavy_Rain\": \"Heavy Rain|Rain Shower|Heavy T-Storm|Heavy Thunderstorms\",\n",
    "    \"Snow\": \"Snow|Sleet|Ice\",\n",
    "    \"Heavy_Snow\": \"Heavy Snow|Heavy Sleet|Heavy Ice Pellets|Snow Showers|Squalls\",\n",
    "    \"Fog\": \"Fog\"\n",
    "}\n",
    "\n",
    "# inizia con colonna vuota\n",
    "expr = F.lit(None)\n",
    "\n",
    "for category, pattern in patterns.items():\n",
    "    expr = F.when(\n",
    "        F.col(\"Weather_Condition\").rlike(f\"(?i){pattern}\"), \n",
    "        category\n",
    "    ).otherwise(expr)\n",
    "    \n",
    "accidents = accidents.withColumn(\"Weather_Condition\", expr)\n",
    "#accidents.select(\"Weather_Condition\").distinct().show(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181ddb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conversione timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "55ef5ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Start_Time_parsed  |\n",
      "+-------------------+\n",
      "|2016-02-08 05:46:00|\n",
      "|2016-02-08 06:07:59|\n",
      "|2016-02-08 06:49:27|\n",
      "|2016-02-08 07:23:34|\n",
      "|2016-02-08 07:39:07|\n",
      "|2016-02-08 07:44:26|\n",
      "|2016-02-08 07:59:35|\n",
      "|2016-02-08 07:59:58|\n",
      "|2016-02-08 08:00:40|\n",
      "|2016-02-08 08:10:04|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accidents = accidents.withColumn(\n",
    "    \"Start_Time_parsed_str\",\n",
    "    F.substring(F.col(\"Start_Time\"), 1, 19)\n",
    ")\n",
    "\n",
    "# Converte in timestamp usando il formato specifico\n",
    "accidents = accidents.withColumn(\n",
    "    \"Start_Time_parsed\",\n",
    "    F.to_timestamp(\"Start_Time_parsed_str\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "accidents.select(\"Start_Time_parsed\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ccbb5",
   "metadata": {},
   "source": [
    "### Gestione date con nuovi dati estrapolati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04d96eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+----+-----------+--------------+\n",
      "|Hour|Day|Month|Year|Part_of_Day|Sunrise_Sunset|\n",
      "+----+---+-----+----+-----------+--------------+\n",
      "|   6|  2|    2|2016|      Night|           Day|\n",
      "|   7|  2|    2|2016|    Morning|           Day|\n",
      "|   7|  2|    2|2016|    Morning|           Day|\n",
      "|   8|  2|    2|2016|    Morning|           Day|\n",
      "|   8|  2|    2|2016|    Morning|           Day|\n",
      "|   8|  2|    2|2016|    Morning|           Day|\n",
      "|   8|  2|    2|2016|    Morning|           Day|\n",
      "|   8|  2|    2|2016|    Morning|           Day|\n",
      "|   9|  2|    2|2016|    Morning|           Day|\n",
      "|   9|  2|    2|2016|    Morning|           Day|\n",
      "+----+---+-----+----+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1) Date features\n",
    "# -----------------------------\n",
    "accidents = accidents.withColumn(\"Hour\",  F.hour(\"Start_Time_parsed\") + 1)\n",
    "accidents = accidents.withColumn(\"Day\",   F.dayofweek(\"Start_Time_parsed\"))  # Spark: 1 = Sunday\n",
    "accidents = accidents.withColumn(\"Month\", F.month(\"Start_Time_parsed\"))\n",
    "accidents = accidents.withColumn(\"Year\",  F.year(\"Start_Time_parsed\"))\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Part_of_Day (cut)\n",
    "# -----------------------------\n",
    "accidents = accidents.withColumn(\n",
    "    \"Part_of_Day\",\n",
    "    F.when(F.col(\"Hour\").between(1, 6),  \"Night\")\n",
    "     .when(F.col(\"Hour\").between(7, 12), \"Morning\")\n",
    "     .when(F.col(\"Hour\").between(13, 18),\"Afternoon\")\n",
    "     .otherwise(\"Evening\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Sunrise_Sunset (Day/Night)\n",
    "# -----------------------------\n",
    "accidents = accidents.withColumn(\n",
    "    \"Sunrise_Sunset\",\n",
    "    F.when(F.col(\"Hour\").between(1, 16), \"Day\")\n",
    "     .otherwise(\"Night\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Show results\n",
    "# -----------------------------\n",
    "accidents.select(\"Hour\", \"Day\", \"Month\", \"Year\", \"Part_of_Day\", \"Sunrise_Sunset\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b3c72b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = accidents.drop(\"Start_Time_parsed\",\"Start_Time\", \"Start_Time_parsed_str\")\n",
    "accidents=accidents.drop(\"Weather_Timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c850b",
   "metadata": {},
   "source": [
    "## Analisys of NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1551bb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Numero totale di righe\\ntotal_rows = accidents.count()\\n\\n# Calcolo missing per ogni colonna\\nmissing_df = accidents.select([\\n    (F.count(F.when(F.col(c).isNull(), 1)) / total_rows * 100)\\n    .alias(c)\\n    for c in accidents.columns\\n])\\n\\n# Converti in pandas per lo stesso stile tabellare\\nmissing_pd = missing_df.toPandas().T.reset_index()\\n\\n# Rename colonne per essere identici al tuo output Pandas\\nmissing_pd.columns = [\"Feature\", \"Missing_Percent(%)\"]\\n\\n# Filtra solo le colonne con missing > 0%\\nmissing_pd = missing_pd[missing_pd[\"Missing_Percent(%)\"] > 0]\\n\\nmissing_pd'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Numero totale di righe\n",
    "total_rows = accidents.count()\n",
    "\n",
    "# Calcolo missing per ogni colonna\n",
    "missing_df = accidents.select([\n",
    "    (F.count(F.when(F.col(c).isNull(), 1)) / total_rows * 100)\n",
    "    .alias(c)\n",
    "    for c in accidents.columns\n",
    "])\n",
    "\n",
    "# Converti in pandas per lo stesso stile tabellare\n",
    "missing_pd = missing_df.toPandas().T.reset_index()\n",
    "\n",
    "# Rename colonne per essere identici al tuo output Pandas\n",
    "missing_pd.columns = [\"Feature\", \"Missing_Percent(%)\"]\n",
    "\n",
    "# Filtra solo le colonne con missing > 0%\n",
    "missing_pd = missing_pd[missing_pd[\"Missing_Percent(%)\"] > 0]\n",
    "\n",
    "missing_pd'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49992f70",
   "metadata": {},
   "source": [
    "## Substitute NaN with \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91d4435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = accidents.withColumn(\n",
    "    \"Street\",\n",
    "    F.coalesce(F.col(\"Street\"), F.lit(\"Unknown\"))\n",
    ")\n",
    "\n",
    "accidents = accidents.withColumn(\n",
    "    \"Zipcode\",\n",
    "    F.coalesce(F.col(\"Zipcode\"), F.lit(\"Unknown\"))\n",
    ")\n",
    "\n",
    "accidents = accidents.withColumn(\n",
    "    \"Weather_Condition\",\n",
    "    F.coalesce(F.col(\"Weather_Condition\"), F.lit(\"Unknown\"))\n",
    ")\n",
    "\n",
    "accidents = accidents.withColumn(\n",
    "    \"Wind_Direction\",\n",
    "    F.coalesce(F.col(\"Wind_Direction\"), F.lit(\"Unknown\"))\n",
    ")\n",
    "accidents = accidents.withColumn(\n",
    "    \"City\",\n",
    "    F.coalesce(F.col(\"City\"), F.lit(\"Unknown\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77ed7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accidents.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f4f92",
   "metadata": {},
   "source": [
    "# Encoding delle categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1fc585",
   "metadata": {},
   "source": [
    "check cat unique values to understand type of encodings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b064a",
   "metadata": {},
   "source": [
    "Unique count of categorical features:\n",
    "**Frequency encoding**\n",
    "- Street: 336306\n",
    "- City: 13678\n",
    "- County: 1871\n",
    "- State: 49\n",
    "- Zipcode: 825094\n",
    "- Airport_Code: 2045\n",
    "\n",
    "**One hot**\n",
    "- Weather_Condition: 8\n",
    "\n",
    "- Timezone: 4\n",
    "\n",
    "**Numerical Encoding**\n",
    "- Part_of_Day 4\n",
    "- Wind_Direction: 11 (there is a sequentiality of data)\n",
    "\n",
    "**Binary encoding**\n",
    "- Sunrise_Sunset: 2\n",
    "- Civil_Twilight: 2\n",
    "- Nautical_Twilight: 2\n",
    "- Astronomical_Twilight: 2\n",
    "- Sunrise_Sunset: 2\n",
    "- Amenity: 2\n",
    "- Bump: 2\n",
    "- Crossing: 2\n",
    "- Give_Way: 2\n",
    "- Junction: 2\n",
    "- No_Exit: 2\n",
    "- Railway: 2\n",
    "- Roundabout: 2\n",
    "- Station: 2\n",
    "- Stop: 2\n",
    "- Traffic_Calming: 2\n",
    "- Traffic_Signal: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2e1c0",
   "metadata": {},
   "source": [
    "##  Numerical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "acae5863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Part_of_Day_idx|\n",
      "+---------------+\n",
      "|            0.0|\n",
      "|            1.0|\n",
      "|            3.0|\n",
      "|            2.0|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Crea l’indexer\n",
    "weather_indexer = StringIndexer(\n",
    "    inputCol=\"Part_of_Day\",\n",
    "    outputCol=\"Part_of_Day_idx\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 2) Applica la trasformazione e aggiorna il DF\n",
    "accidents = weather_indexer.fit(accidents).transform(accidents)\n",
    "\n",
    "\n",
    "# 4) Mostra i valori distinti dell’indice\n",
    "accidents.select(\"Part_of_Day_idx\").distinct().show(150)\n",
    "\n",
    "accidents=accidents.drop(\"Part_of_Day\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cfb2c569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Wind_Direction_idx|\n",
      "+------------------+\n",
      "|               8.0|\n",
      "|               0.0|\n",
      "|               7.0|\n",
      "|               1.0|\n",
      "|               4.0|\n",
      "|               3.0|\n",
      "|               2.0|\n",
      "|              10.0|\n",
      "|               6.0|\n",
      "|               5.0|\n",
      "|               9.0|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wind_indexer = StringIndexer(\n",
    "    inputCol=\"Wind_Direction\",\n",
    "    outputCol=\"Wind_Direction_idx\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "accidents =wind_indexer.fit(accidents).transform(accidents)\n",
    "accidents.select(\"Wind_Direction_idx\").distinct().show(150)\n",
    "\n",
    "accidents=accidents.drop(\"Wind_Direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d260f4fd",
   "metadata": {},
   "source": [
    "## Frequency encoding \n",
    "Used for high cardinality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4168ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_card_cols = [\"Street\", \"City\", \"Zipcode\", \"County\", \"Airport_Code\",\"State\"]\n",
    "\n",
    "\n",
    "def freq_encode(df, col):\n",
    "    freq = df.groupBy(col).count().withColumnRenamed(\"count\", f\"{col}_freq\")\n",
    "    df = df.join(freq, on=col, how=\"left\")\n",
    "    return df.drop(col)\n",
    "\n",
    "for c in high_card_cols:\n",
    "    accidents = freq_encode(accidents, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c0b56",
   "metadata": {},
   "source": [
    "## One hot encoding \n",
    "for low cardinality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0f7dbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "small_cat = [\"Weather_Condition\", \"Timezone\"]\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in small_cat\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}\")\n",
    "    for c in small_cat\n",
    "]\n",
    "\n",
    "accidents=accidents.drop(\"Weather_Condition\",\"Timezone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67fbf45",
   "metadata": {},
   "source": [
    "## binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f104225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "binary_cols = [\n",
    "    \"Sunrise_Sunset\",\n",
    "    \"Civil_Twilight\",\n",
    "    \"Nautical_Twilight\",\n",
    "    \"Astronomical_Twilight\",\n",
    "    \"Amenity\",\n",
    "    \"Bump\",\n",
    "    \"Crossing\",\n",
    "    \"Give_Way\",\n",
    "    \"Junction\",\n",
    "    \"No_Exit\",\n",
    "    \"Railway\",\n",
    "    \"Roundabout\",\n",
    "    \"Station\",\n",
    "    \"Stop\",\n",
    "    \"Traffic_Calming\",\n",
    "    \"Traffic_Signal\"\n",
    "]\n",
    "\n",
    "for c in binary_cols:\n",
    "    accidents = accidents.withColumn(\n",
    "        f\"{c}_bin\",\n",
    "        F.when(\n",
    "            F.lower(F.col(c)).isin(\"true\", \"yes\", \"1\", \"day\"), \n",
    "            1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "\n",
    "# Drop colonne originali\n",
    "accidents = accidents.drop(*binary_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e5df9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accidents.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89caac0",
   "metadata": {},
   "source": [
    "## Corr matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01e423",
   "metadata": {},
   "source": [
    "|correlazione| < 0.05 → quasi zero → irrilevanti\n",
    "\n",
    "|correlazione| < 0.01 → rumore puro → da rimuove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "28e63e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Severity</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Street_freq</td>\n",
       "      <td>0.129758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Zipcode_freq</td>\n",
       "      <td>0.086977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Start_Lat</td>\n",
       "      <td>0.069060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Start_Lng</td>\n",
       "      <td>0.052862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature  correlation\n",
       "0       Severity     1.000000\n",
       "16   Street_freq     0.129758\n",
       "18  Zipcode_freq     0.086977\n",
       "1      Start_Lat     0.069060\n",
       "2      Start_Lng     0.052862"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = \"severity\"\n",
    "\n",
    "correlations = []\n",
    "for c in accidents.columns:\n",
    "    if c != target:\n",
    "        corr = accidents.stat.corr(c, target)\n",
    "        correlations.append((c, corr))\n",
    "\n",
    "corr_df = pd.DataFrame(correlations, columns=[\"feature\", \"correlation\"]).dropna()\n",
    "corr_df = corr_df.sort_values(\"correlation\", ascending=False)\n",
    "corr_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "384d9e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corr_df[\"abs_correlation\"] = corr_df[\"correlation\"].abs()\\ncorr_df.sort_values(\"abs_correlation\", ascending=False)'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''corr_df[\"abs_correlation\"] = corr_df[\"correlation\"].abs()\n",
    "corr_df.sort_values(\"abs_correlation\", ascending=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9aefa209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.figure(figsize=(20, 6))\\nsns.barplot(\\n    data=corr_df,\\n    x=\"feature\",\\n    y=\"correlation\",\\n    palette=\"coolwarm\"\\n)\\nplt.xticks(rotation=90)\\nplt.title(\"Correlation of ALL features with Severity\")\\nplt.tight_layout()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''plt.figure(figsize=(20, 6))\n",
    "sns.barplot(\n",
    "    data=corr_df,\n",
    "    x=\"feature\",\n",
    "    y=\"correlation\",\n",
    "    palette=\"coolwarm\"\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Correlation of ALL features with Severity\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ca74a",
   "metadata": {},
   "source": [
    "## Drop features with corr near 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c822a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents=accidents.drop(\"Civil_Twilight_bin\",\"Nautical_Twilight_bin\",\"Astronomical_Twilight_bin\",\n",
    "                         \"Traffic_Calming_bin\",\"Bump_bin\",\"Give_Way_bin\",\"Day\",\"Visibility(mi)\",\"Roundabout_bin\",\n",
    "                            \"Wind_Direction_idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0a64e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    \"Civil_Twilight_bin\", \"Nautical_Twilight_bin\", \"Astronomical_Twilight_bin\",\n",
    "    \"Traffic_Calming_bin\", \"Bump_bin\", \"Give_Way_bin\",\n",
    "    \"Day\", \"Visibility(mi)\", \"Roundabout_bin\", \"Wind_Direction_idx\"\n",
    "]\n",
    "\n",
    "corr_df = corr_df[~corr_df[\"feature\"].isin(to_drop)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9b28607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.figure(figsize=(20, 6))\\nsns.barplot(\\n    data=corr_df,\\n    x=\"feature\",\\n    y=\"correlation\",\\n    palette=\"coolwarm\"\\n)\\nplt.xticks(rotation=90)\\nplt.title(\"Correlation of ALL features with Severity\")\\nplt.tight_layout()\\nplt.show()\\ncorr_df.sort_values(\"abs_correlation\", ascending=False)'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''plt.figure(figsize=(20, 6))\n",
    "sns.barplot(\n",
    "    data=corr_df,\n",
    "    x=\"feature\",\n",
    "    y=\"correlation\",\n",
    "    palette=\"coolwarm\"\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Correlation of ALL features with Severity\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "corr_df.sort_values(\"abs_correlation\", ascending=False)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb4b09",
   "metadata": {},
   "source": [
    "# Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78778a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "counts = accidents.groupBy(\"severity\").count().toPandas()\n",
    "total = counts[\"count\"].sum()\n",
    "\n",
    "weights = {row[\"severity\"]: total / row[\"count\"] for _, row in counts.iterrows()}\n",
    "print(weights)\n",
    "\n",
    "accidents = accidents.withColumn(\n",
    "    \"classWeightCol\",\n",
    "    F.when(F.col(\"severity\") == 1, weights[1])\n",
    "     .when(F.col(\"severity\") == 2, weights[2])\n",
    "     .when(F.col(\"severity\") == 3, weights[3])\n",
    "     .when(F.col(\"severity\") == 4, weights[4])\n",
    ")'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b8a69c",
   "metadata": {},
   "source": [
    "# Stratified Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "245c2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Aggiungi una colonna random per il sort'''\n",
    "df = accidents.withColumn(\"rand\", F.rand(seed=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "29092b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ordina per severity e rand (stratified shuffle)\n",
    "\n",
    "Questo garantisce che le classi siano mescolate ma mantenendo proporzioni identiche'''\n",
    "df = df.orderBy(\"severity\", \"rand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ada03c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split normale (è STRATIFICATO perché il dataset è ordinato!)\n",
    "\n",
    "Con tantissimi record per classe, questo funziona perfettamente.'''\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c1949844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_counts = Counter(train.select(\"severity\").rdd.map(lambda r: r[0]).takeSample(False, 500000))\\ntest_counts  = Counter(test.select(\"severity\").rdd.map(lambda r: r[0]).takeSample(False, 500000))\\n\\nprint(\"TRAIN (sampled):\", train_counts)\\nprint(\"TEST  (sampled):\", test_counts)'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "'''train_counts = Counter(train.select(\"severity\").rdd.map(lambda r: r[0]).takeSample(False, 500000))\n",
    "test_counts  = Counter(test.select(\"severity\").rdd.map(lambda r: r[0]).takeSample(False, 500000))\n",
    "\n",
    "print(\"TRAIN (sampled):\", train_counts)\n",
    "print(\"TEST  (sampled):\", test_counts)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "34829b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = [\n",
    "    \"Start_Lat\", \"Start_Lng\", \"Temperature(F)\", \"Wind_Chill(F)\", \"Humidity(%)\",\n",
    "    \"Pressure(in)\", \"Wind_Speed(mph)\", \"Precipitation(in)\",\n",
    "    \"Hour\", \"Month\", \"Year\",\n",
    "    \"Part_of_Day_idx\", \"Street_freq\", \"City_freq\", \"Zipcode_freq\",\n",
    "    \"County_freq\", \"Airport_Code_freq\", \"State_freq\",\n",
    "    \"Sunrise_Sunset_bin\", \"Amenity_bin\", \"Crossing_bin\", \"Junction_bin\",\n",
    "    \"No_Exit_bin\", \"Railway_bin\", \"Station_bin\", \"Stop_bin\",\n",
    "    \"Traffic_Signal_bin\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"   # <-- salta le righe che contengono null\n",
    ")\n",
    "\n",
    "train_assembled = assembler.transform(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b31a5",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "65822559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d7437",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d6761983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 211.56 seconds\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Severity\",\n",
    "    #weightCol=\"classWeightCol\",   # <-- importantissimo\n",
    "    numTrees=10,\n",
    "    maxDepth=12,\n",
    "    maxBins=32,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    rf\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "rf_model = pipeline.fit(train)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "with open(\"rf_model_time.txt\", \"w\") as f:\n",
    "    f.write(f\"Training time: {end_time - start_time:.2f} seconds\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e38da050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.8072335776242325\n",
      "Accuracy = 0.834787565031444\n"
     ]
    }
   ],
   "source": [
    "predictions = rf_model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(\"F1 =\", f1)\n",
    "\n",
    "acc = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "print(\"Accuracy =\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13308313",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f3f29c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 141.05464482307434 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Severity\",\n",
    "    maxIter=50,\n",
    "    regParam=0.0,\n",
    "    elasticNetParam=0.0\n",
    ")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    lr\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "lr_model = pipeline.fit(train)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time} seconds\")  \n",
    "  \n",
    "with open(\"lr_model_time.txt\", \"w\") as f:\n",
    "    f.write(f\"Training time: {end_time - start_time:.2f} seconds\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a643ac5",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3f5bdbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.7520276736954681\n",
      "Accuracy = 0.8014088426121663\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(\"F1 =\", f1)\n",
    "\n",
    "acc = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "print(\"Accuracy =\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
